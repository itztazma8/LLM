{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1152f118",
   "metadata": {},
   "source": [
    "##### Token Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ae198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('PoetryFoundationData.csv')\n",
    "df.columns\n",
    "\n",
    "col1=df['Poem']\n",
    "entry=\"\"\n",
    "for i in range(0,21):\n",
    "    for j in col1[i]:\n",
    "        entry=entry+j\n",
    "\n",
    "import re\n",
    "text=re.split(r'(\\s)', entry)\n",
    "#Removing spaces\n",
    "\n",
    "new_list=[]\n",
    "for i in text:\n",
    "    new=i.strip()\n",
    "    if new!=\"\":\n",
    "        new_list.append(new)\n",
    "\n",
    "#print(new_list)\n",
    "new_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349032a0",
   "metadata": {},
   "source": [
    "##### Token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552a2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "id={}\n",
    "for i in range(len(new_list)):\n",
    "    id.update({new_list[i]:i})\n",
    "\n",
    "#print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fef24e",
   "metadata": {},
   "source": [
    "##### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846d2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a big Dog \n"
     ]
    }
   ],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, dic):\n",
    "        self.dic=dic\n",
    "\n",
    "        dic1={}\n",
    "        for i,j in dic.items():\n",
    "            dic1.update({j:i})\n",
    "\n",
    "        self.inverse_dic=dic1\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        text=re.split(r'([,:?;]|\\s)', txt)\n",
    "        id=[]\n",
    "        for i in text:\n",
    "            j=i.strip()\n",
    "            if j!=\"\":\n",
    "                id.append(self.dic[j])\n",
    "        \n",
    "        return id\n",
    "    \n",
    "    def decode(self, id):\n",
    "        st=\"\"\n",
    "        for i in range(len(id)):\n",
    "            if id==len(id)-1:\n",
    "                st=st+self.inverse_dic[id[i]]\n",
    "            else:\n",
    "                st=st+self.inverse_dic[id[i]]+\" \"\n",
    "        \n",
    "        return st\n",
    "\n",
    "token1=tokenizer(id)\n",
    "entry=token1.encode(\"It's a big Dog\")\n",
    "final=token1.decode(entry)\n",
    "print(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d5a40",
   "metadata": {},
   "source": [
    "##### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a577c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love BMW\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer1=tiktoken.get_encoding(\"gpt2\")\n",
    "codes=tokenizer1.encode(\"I love BMW\")\n",
    "text=tokenizer1.decode(codes)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b53f6",
   "metadata": {},
   "source": [
    "##### Sliding Window Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0b43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are the start of the week\n",
      "\n",
      "tensor([[ 342,  582],\n",
      "        [2694, 2396],\n",
      "        [1956, 2694]])\n",
      "tensor([[ 582, 2694],\n",
      "        [2396, 1956],\n",
      "        [2694, 2975]])\n"
     ]
    }
   ],
   "source": [
    "text_to_encode=col1[5]\n",
    "limit=32\n",
    "actual=text_to_encode[0:limit]\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in actual.split(\" \"):\n",
    "    Y.append(i.strip())\n",
    "    X.append(i.strip())\n",
    "\n",
    "\n",
    "Y.remove(Y[0])\n",
    "X.remove(X[-1])\n",
    "\n",
    "print(actual+\"\\n\")\n",
    "\n",
    "X_id=[]\n",
    "Y_id=[]\n",
    "for i in X:\n",
    "    X_id.append(id[i])\n",
    "\n",
    "for i in Y:\n",
    "    Y_id.append(id[i])\n",
    "\n",
    "import torch\n",
    "\n",
    "x_tensor=[]\n",
    "y_tensor=[]\n",
    "\n",
    "for i in range(0, len(X_id), 2):\n",
    "    li=[]\n",
    "    li.append(X_id[i])\n",
    "    li.append(X_id[i+1])\n",
    "\n",
    "    li1=[]\n",
    "    li1.append(Y_id[i])\n",
    "    li1.append(Y_id[i+1])\n",
    "\n",
    "    x_tensor.append(li)\n",
    "    y_tensor.append(li1)\n",
    "\n",
    "x_tensor=torch.tensor(x_tensor)\n",
    "y_tensor=torch.tensor(y_tensor)\n",
    "print(x_tensor)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffba8d8",
   "metadata": {},
   "source": [
    "##### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66602ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3977, -0.9286, -1.3481],\n",
      "         [-1.3189, -0.8629, -2.0889]],\n",
      "\n",
      "        [[ 0.5474,  0.0208,  1.4589],\n",
      "         [-0.7708,  0.5540,  0.6492]],\n",
      "\n",
      "        [[ 1.6639,  0.0560,  0.2612],\n",
      "         [ 0.5474,  0.0208,  1.4589]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding=torch.nn.Embedding(len(new_list), 3)\n",
    "#print(embedding.weight)\n",
    "embebbed_layer=embedding(x_tensor)\n",
    "print(embebbed_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1500e",
   "metadata": {},
   "source": [
    "##### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f03678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.8472e-01, 5.0704e-02, 2.5375e-03, 7.2986e-04, 5.8772e-02, 2.5375e-03],\n",
      "        [6.2161e-03, 9.9321e-01, 2.3893e-05, 4.6567e-04, 6.4876e-05, 2.3893e-05],\n",
      "        [1.0403e-02, 7.9895e-04, 3.9998e-01, 6.0317e-02, 1.2852e-01, 3.9998e-01],\n",
      "        [1.0554e-02, 5.4925e-02, 2.1275e-01, 4.6686e-01, 4.2159e-02, 2.1275e-01],\n",
      "        [2.1594e-01, 1.9443e-03, 1.1518e-01, 1.0712e-02, 5.4104e-01, 1.1518e-01],\n",
      "        [1.0403e-02, 7.9895e-04, 3.9998e-01, 6.0317e-02, 1.2852e-01, 3.9998e-01]])\n",
      "\n",
      "--------------Context Vector---------------\n",
      "\n",
      "tensor([[ 1.2697, -0.8615, -1.2754],\n",
      "        [-1.3015, -0.8625, -2.0827],\n",
      "        [ 0.6187,  0.0469,  1.2241],\n",
      "        [-0.1145,  0.2127,  0.8059],\n",
      "        [ 1.3174, -0.1612,  0.1892],\n",
      "        [ 0.6187,  0.0469,  1.2241]])\n"
     ]
    }
   ],
   "source": [
    "new_dim=[]\n",
    "for i in embebbed_layer:\n",
    "    for j in i:\n",
    "        new_dim.append(j.tolist())   \n",
    "\n",
    "new_dim=torch.tensor(new_dim)\n",
    "\n",
    "attention=torch.empty(6,6)\n",
    "\n",
    "\n",
    "for i,j in enumerate(new_dim):\n",
    "    for k,l in enumerate(new_dim):\n",
    "        attention[i,k]=torch.dot(j,l)\n",
    "\n",
    "attention=torch.softmax(attention, dim=1)\n",
    "print(attention)\n",
    "\n",
    "context=attention@new_dim\n",
    "print(\"\\n--------------Context Vector---------------\\n\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783796c",
   "metadata": {},
   "source": [
    "##### Attention Mechanism With Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba6fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------ATTENTION SCORES-------------\n",
      "\n",
      "tensor([[0.1902, 0.2599, 0.1337, 0.1485, 0.1340, 0.1337],\n",
      "        [0.0555, 0.9390, 0.0010, 0.0023, 0.0013, 0.0010],\n",
      "        [0.0317, 0.0078, 0.2761, 0.1798, 0.2284, 0.2761],\n",
      "        [0.1584, 0.1391, 0.1772, 0.1692, 0.1789, 0.1772],\n",
      "        [0.0335, 0.0078, 0.2746, 0.1737, 0.2359, 0.2746],\n",
      "        [0.0317, 0.0078, 0.2761, 0.1798, 0.2284, 0.2761]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "-----------CONTEXT VECTOR-------------\n",
      "\n",
      "tensor([[-0.1446, -0.0892],\n",
      "        [-1.6522, -1.6847],\n",
      "        [ 0.5177,  0.5820],\n",
      "        [ 0.1583,  0.2235],\n",
      "        [ 0.5267,  0.5932],\n",
      "        [ 0.5177,  0.5820]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Input embebbed layer\n",
    "input=embebbed_layer.reshape(-1, embebbed_layer.shape[2])\n",
    "\n",
    "\n",
    "torch.manual_seed(150)\n",
    "query=torch.nn.Parameter(torch.rand(3,2), requires_grad=False)\n",
    "key=torch.nn.Parameter(torch.rand(3,2), requires_grad=False)\n",
    "value=torch.nn.Parameter(torch.rand(3,2), requires_grad=False)\n",
    "\n",
    "query_vector=input@query\n",
    "key_vector=input@key\n",
    "value_vector=input@value \n",
    "\n",
    "# Attention Score Calculation\n",
    "attention_scores=query_vector @ key_vector.T\n",
    "\n",
    "#Scaling and Normalization\n",
    "key_shape=key_vector.shape[-1]\n",
    "attention_scores_final=torch.softmax((attention_scores/(key_shape**0.5)), dim=1)\n",
    "print(\"-----------ATTENTION SCORES-------------\\n\")\n",
    "print(attention_scores_final)\n",
    "#Context Vector\n",
    "context_vector=attention_scores_final @ value_vector\n",
    "print(\"-----------CONTEXT VECTOR-------------\\n\")\n",
    "print(context_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4a8b7",
   "metadata": {},
   "source": [
    "##### Causal Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84da8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------ATTENTION SCORES-------------\n",
      "\n",
      "tensor([[1.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5812, 1.0855, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5255, 0.5166, 0.6246, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4159, 0.0000, 0.0000, 0.4191, 0.0000, 0.0000],\n",
      "        [0.3072, 0.0000, 0.0000, 0.0000, 0.3544, 0.0000],\n",
      "        [0.2518, 0.2475, 0.0000, 0.2795, 0.0000, 0.2993]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "-----------CONTEXT VECTOR-------------\n",
      "\n",
      "tensor([[ 0.3135,  0.7107],\n",
      "        [-1.8154, -1.7297],\n",
      "        [-0.5410, -0.3885],\n",
      "        [ 0.0499,  0.0904],\n",
      "        [ 0.5131,  0.6420],\n",
      "        [-0.2781, -0.2441]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using Previous Attention Vector\n",
    "\n",
    "masked_matrix=torch.tril(torch.ones(6,6))\n",
    "new_attention=attention_scores_final * masked_matrix\n",
    "print(\"-----------ATTENTION SCORES-------------\\n\")\n",
    "\n",
    "#Normalization\n",
    "row=new_attention.sum(dim=1, keepdim=True)\n",
    "masked_attention=new_attention/row\n",
    "\n",
    "#Fixing Normalization Problem\n",
    "fix=torch.triu(torch.ones(6,6), diagonal=1)\n",
    "fixed_attention=attention_scores_final.masked_fill(fix.bool(), -torch.inf)\n",
    "\n",
    "#Softmax with scaling\n",
    "attention_score=torch.softmax(fixed_attention/(key_shape**0.5), dim=1)\n",
    "\n",
    "#Dropout Phase\n",
    "dropout=torch.nn.Dropout(0.4)\n",
    "attention_score_1=dropout(attention_score)\n",
    "\n",
    "print(attention_score_1)\n",
    "print(\"-----------CONTEXT VECTOR-------------\\n\")\n",
    "context_vector_causal=attention_score_1 @ value_vector\n",
    "print(context_vector_causal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ccd0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, dropout, dim_in, dim_out, context_size, bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.out=dim_out\n",
    "        self.query=nn.Linear(dim_in,dim_out, bias=bias)\n",
    "        self.key=nn.Linear(dim_in,dim_out, bias=bias)\n",
    "        self.value=nn.Linear(dim_in,dim_out, bias=bias)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_size, context_size)), diagonal=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query_vector=self.query(x)\n",
    "        key_vector=self.key(x)\n",
    "        value_vector=self.value(x)\n",
    "        outside, border, inside=x.shape\n",
    "\n",
    "        attention_scores=query_vector @ key_vector.transpose(1,2)\n",
    "        \n",
    "        scores = attention_scores.masked_fill(self.mask.bool()[:border, :border], -torch.inf)\n",
    "        attention_1=torch.softmax(scores/(key_vector.shape[-1]**0.5), dim=-1)\n",
    "\n",
    "        attention_score=self.dropout(attention_1)\n",
    "        context_vector=attention_score @ value_vector\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18db62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, d_in, d_out, bias, dropout, context_length, iterations):\n",
    "        super().__init__()\n",
    "        self.head=nn.ModuleList()\n",
    "        for i in range(iterations):\n",
    "            self.head.append(Head(d_in, d_out, dropout, context_length, bias=False))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        output=[]\n",
    "        for h in self.head:\n",
    "            output.append(h(x))\n",
    "        \n",
    "        final=torch.cat(output, dim=-1)\n",
    "        return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10722ba",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a748dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1921e-07], grad_fn=<MeanBackward1>)\n",
      "tensor([1.0000], grad_fn=<VarBackward0>)\n",
      "\n",
      "\n",
      "-------Normalised Matrix-------\n",
      "tensor([ 0.1689,  0.9048, -1.0737], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test=embebbed_layer[0,1]\n",
    "mean=test.mean(dim=-1, keepdim=True)\n",
    "var=test.var(dim=-1, keepdim=True)\n",
    "test=(test-mean)/torch.sqrt(var)\n",
    "\n",
    "v1=test.mean(dim=-1, keepdim=True)\n",
    "v2=test.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(\"\\n\")\n",
    "print(\"-------Normalised Matrix-------\")\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
