{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1152f118",
   "metadata": {},
   "source": [
    "##### Token Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ae198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('PoetryFoundationData.csv')\n",
    "df.columns\n",
    "\n",
    "col1=df['Poem']\n",
    "entry=\"\"\n",
    "for i in range(0,21):\n",
    "    for j in col1[i]:\n",
    "        entry=entry+j\n",
    "\n",
    "import re\n",
    "text=re.split(r'(\\s)', entry)\n",
    "#Removing spaces\n",
    "\n",
    "new_list=[]\n",
    "for i in text:\n",
    "    new=i.strip()\n",
    "    if new!=\"\":\n",
    "        new_list.append(new)\n",
    "\n",
    "#print(new_list)\n",
    "new_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349032a0",
   "metadata": {},
   "source": [
    "##### Token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552a2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "id={}\n",
    "for i in range(len(new_list)):\n",
    "    id.update({new_list[i]:i})\n",
    "\n",
    "#print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fef24e",
   "metadata": {},
   "source": [
    "##### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846d2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a big Dog \n"
     ]
    }
   ],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, dic):\n",
    "        self.dic=dic\n",
    "\n",
    "        dic1={}\n",
    "        for i,j in dic.items():\n",
    "            dic1.update({j:i})\n",
    "\n",
    "        self.inverse_dic=dic1\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        text=re.split(r'([,:?;]|\\s)', txt)\n",
    "        id=[]\n",
    "        for i in text:\n",
    "            j=i.strip()\n",
    "            if j!=\"\":\n",
    "                id.append(self.dic[j])\n",
    "        \n",
    "        return id\n",
    "    \n",
    "    def decode(self, id):\n",
    "        st=\"\"\n",
    "        for i in range(len(id)):\n",
    "            if id==len(id)-1:\n",
    "                st=st+self.inverse_dic[id[i]]\n",
    "            else:\n",
    "                st=st+self.inverse_dic[id[i]]+\" \"\n",
    "        \n",
    "        return st\n",
    "\n",
    "token1=tokenizer(id)\n",
    "entry=token1.encode(\"It's a big Dog\")\n",
    "final=token1.decode(entry)\n",
    "print(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d5a40",
   "metadata": {},
   "source": [
    "##### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a577c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love BMW\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer1=tiktoken.get_encoding(\"gpt2\")\n",
    "codes=tokenizer1.encode(\"I love BMW\")\n",
    "text=tokenizer1.decode(codes)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b53f6",
   "metadata": {},
   "source": [
    "##### Sliding Window Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0b43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are the start of the week\n",
      "\n",
      "tensor([[ 342,  582],\n",
      "        [2694, 2396],\n",
      "        [1956, 2694]])\n",
      "tensor([[ 582, 2694],\n",
      "        [2396, 1956],\n",
      "        [2694, 2975]])\n"
     ]
    }
   ],
   "source": [
    "text_to_encode=col1[5]\n",
    "limit=32\n",
    "actual=text_to_encode[0:limit]\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in actual.split(\" \"):\n",
    "    Y.append(i.strip())\n",
    "    X.append(i.strip())\n",
    "\n",
    "\n",
    "Y.remove(Y[0])\n",
    "X.remove(X[-1])\n",
    "\n",
    "print(actual+\"\\n\")\n",
    "\n",
    "X_id=[]\n",
    "Y_id=[]\n",
    "for i in X:\n",
    "    X_id.append(id[i])\n",
    "\n",
    "for i in Y:\n",
    "    Y_id.append(id[i])\n",
    "\n",
    "import torch\n",
    "\n",
    "x_tensor=[]\n",
    "y_tensor=[]\n",
    "\n",
    "for i in range(0, len(X_id), 2):\n",
    "    li=[]\n",
    "    li.append(X_id[i])\n",
    "    li.append(X_id[i+1])\n",
    "\n",
    "    li1=[]\n",
    "    li1.append(Y_id[i])\n",
    "    li1.append(Y_id[i+1])\n",
    "\n",
    "    x_tensor.append(li)\n",
    "    y_tensor.append(li1)\n",
    "\n",
    "x_tensor=torch.tensor(x_tensor)\n",
    "y_tensor=torch.tensor(y_tensor)\n",
    "print(x_tensor)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffba8d8",
   "metadata": {},
   "source": [
    "##### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66602ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.3678,  0.2255, -0.8966],\n",
      "         [ 0.0620, -0.1065,  0.2757]],\n",
      "\n",
      "        [[ 0.8565,  0.2817, -0.9101],\n",
      "         [ 0.1733, -0.2974, -1.3095]],\n",
      "\n",
      "        [[-2.3439,  0.8081,  0.2367],\n",
      "         [ 0.8565,  0.2817, -0.9101]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding=torch.nn.Embedding(len(new_list), 3)\n",
    "#print(embedding.weight)\n",
    "embebbed_layer=embedding(x_tensor)\n",
    "print(embebbed_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1500e",
   "metadata": {},
   "source": [
    "##### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f03678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.3829e-01, 1.2952e-03, 2.6858e-02, 6.6886e-03, 5.5341e-06, 2.6858e-02],\n",
      "        [1.7163e-01, 2.1291e-01, 1.5476e-01, 1.4133e-01, 1.6461e-01, 1.5476e-01],\n",
      "        [5.5354e-01, 2.4071e-02, 1.5604e-01, 1.0620e-01, 4.1101e-03, 1.5604e-01],\n",
      "        [2.4061e-01, 3.8369e-02, 1.8536e-01, 3.3002e-01, 2.0272e-02, 1.8536e-01],\n",
      "        [7.6103e-06, 1.7083e-03, 2.7424e-04, 7.7494e-04, 9.9696e-01, 2.7424e-04],\n",
      "        [5.5354e-01, 2.4071e-02, 1.5604e-01, 1.0620e-01, 4.1101e-03, 1.5604e-01]])\n",
      "\n",
      "--------------Context Vector---------------\n",
      "\n",
      "tensor([[ 2.2689,  0.2246, -0.8986],\n",
      "        [ 0.3234,  0.1942, -0.5230],\n",
      "        [ 1.5882,  0.1819, -0.9118],\n",
      "        [ 0.8993,  0.0728, -0.9699],\n",
      "        [-2.3361,  0.8054,  0.2350],\n",
      "        [ 1.5882,  0.1819, -0.9118]])\n"
     ]
    }
   ],
   "source": [
    "new_dim=[]\n",
    "for i in embebbed_layer:\n",
    "    for j in i:\n",
    "        new_dim.append(j.tolist())   \n",
    "\n",
    "new_dim=torch.tensor(new_dim)\n",
    "\n",
    "attention=torch.empty(6,6)\n",
    "\n",
    "\n",
    "for i,j in enumerate(new_dim):\n",
    "    for k,l in enumerate(new_dim):\n",
    "        attention[i,k]=torch.dot(j,l)\n",
    "\n",
    "attention=torch.softmax(attention, dim=1)\n",
    "print(attention)\n",
    "\n",
    "context=attention@new_dim\n",
    "print(\"\\n--------------Context Vector---------------\\n\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783796c",
   "metadata": {},
   "source": [
    "##### Attention Mechanism With Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba6fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------ATTENTION SCORES-------------\n",
      "\n",
      "tensor([[0.2937, 0.1581, 0.1730, 0.0697, 0.1326, 0.1730],\n",
      "        [0.1796, 0.1684, 0.1682, 0.1454, 0.1702, 0.1682],\n",
      "        [0.1863, 0.1646, 0.1709, 0.1571, 0.1504, 0.1709],\n",
      "        [0.1041, 0.1525, 0.1475, 0.2888, 0.1595, 0.1475],\n",
      "        [0.0870, 0.1421, 0.1380, 0.3501, 0.1448, 0.1380],\n",
      "        [0.1863, 0.1646, 0.1709, 0.1571, 0.1504, 0.1709]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "-----------CONTEXT VECTOR-------------\n",
      "\n",
      "tensor([[0.7210, 0.7397],\n",
      "        [0.4409, 0.4296],\n",
      "        [0.4768, 0.4737],\n",
      "        [0.2474, 0.2297],\n",
      "        [0.2036, 0.1891],\n",
      "        [0.4768, 0.4737]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Input embebbed layer\n",
    "input=embebbed_layer.reshape(-1, embebbed_layer.shape[2])\n",
    "\n",
    "\n",
    "torch.manual_seed(150)\n",
    "query=torch.nn.Parameter(torch.rand(3,2), requires_grad=False)\n",
    "key=torch.nn.Parameter(torch.rand(3,2), requires_grad=False)\n",
    "value=torch.nn.Parameter(torch.rand(3,2), requires_grad=False)\n",
    "\n",
    "query_vector=input@query\n",
    "key_vector=input@key\n",
    "value_vector=input@value \n",
    "\n",
    "# Attention Score Calculation\n",
    "attention_scores=query_vector @ key_vector.T\n",
    "\n",
    "#Scaling and Normalization\n",
    "key_shape=key_vector.shape[-1]\n",
    "attention_scores_final=torch.softmax((attention_scores/(key_shape**0.5)), dim=1)\n",
    "print(\"-----------ATTENTION SCORES-------------\\n\")\n",
    "print(attention_scores_final)\n",
    "#Context Vector\n",
    "context_vector=attention_scores_final @ value_vector\n",
    "print(\"-----------CONTEXT VECTOR-------------\\n\")\n",
    "print(context_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4a8b7",
   "metadata": {},
   "source": [
    "##### Causal Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84da8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------ATTENTION SCORES-------------\n",
      "\n",
      "tensor([[1.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8366, 0.8300, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5604, 0.0000, 0.5544, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3963, 0.0000, 0.4087, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3246, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2816, 0.0000, 0.2786, 0.2759, 0.2746, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "-----------CONTEXT VECTOR-------------\n",
      "\n",
      "tensor([[3.2372, 3.5236],\n",
      "        [1.5871, 1.7543],\n",
      "        [1.5730, 1.6797],\n",
      "        [1.1269, 1.2026],\n",
      "        [0.2837, 0.2898],\n",
      "        [0.4707, 0.4397]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using Previous Attention Vector\n",
    "\n",
    "masked_matrix=torch.tril(torch.ones(6,6))\n",
    "new_attention=attention_scores_final * masked_matrix\n",
    "print(\"-----------ATTENTION SCORES-------------\\n\")\n",
    "\n",
    "#Normalization\n",
    "row=new_attention.sum(dim=1, keepdim=True)\n",
    "masked_attention=new_attention/row\n",
    "\n",
    "#Fixing Normalization Problem\n",
    "fix=torch.triu(torch.ones(6,6), diagonal=1)\n",
    "fixed_attention=attention_scores_final.masked_fill(fix.bool(), -torch.inf)\n",
    "\n",
    "#Softmax with scaling\n",
    "attention_score=torch.softmax(fixed_attention/(key_shape**0.5), dim=1)\n",
    "\n",
    "#Dropout Phase\n",
    "dropout=torch.nn.Dropout(0.4)\n",
    "attention_score_1=dropout(attention_score)\n",
    "\n",
    "print(attention_score_1)\n",
    "print(\"-----------CONTEXT VECTOR-------------\\n\")\n",
    "context_vector_causal=attention_score_1 @ value_vector\n",
    "print(context_vector_causal)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
